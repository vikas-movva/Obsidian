For each activity, give a PEAS description of the task environment and characterize it in terms of the properties listed in Section 2.3.2 of the textbook. 
- Playing soccer. 
- Exploring the subsurface oceans of Titan.
- Shopping for used AI books on the Internet.
- Playing a tennis match.
- Practicing tennis against a wall.
- Performing a high jump.
- Knitting a sweater.
- Bidding on an item at an auction.

### 2.3.2 Properties of task environments 
The range of task environments that might arise in AI is obviously vast. We can, however, identify a fairly small number of dimensions along which task environments can be categorized. These dimensions determine, to a large extent, the appropriate agent design and the applicability of each of the principal families of techniques for agent implementation. First, we list the dimensions, then we analyze several task environments to illustrate the ideas. The definitions here are informal; later chapters provide more precise statements and examples of each kind of environment.


| Agent Type                      | Performance Measure                 | Environment                      | Actuators                                                     | Sensors                                                 |
| ------------------------------- | ----------------------------------- | -------------------------------- | ------------------------------------------------------------- | ------------------------------------------------------- |
| Medical diagnosis system        | Healthy patient, reduced costs      | Patient, hospital, staff         | Display of questions, tests, diagnoses, treatments, referrals | Keyboard entry of symptoms, findings, patient’s answers |
| Satellite image analysis system | Correct image categorization        | Downlink from orbiting satellite | Display of scene categorization                               | Color pixel arrays                                      |
| Part-picking robot              | Percentage of parts in correct bins | Conveyor belt with parts; bins   | Jointed arm and hand                                          | Camera, joint angle sensors                             |
| Refinery controlle              | Purity, yield, safety               | Refinery, operators              | Valves, pumps, heaters, displays                              | Temperature, pressure, chemical sensors                 |
| Interactive English tutor       | Student’s score on test             | Set of students, testing agency  | Display of exercises, suggestions, corrections                | Keyboard entry                                          |

>Figure 2.5 Examples of agent types and their PEAS descriptions

Fully observable vs. partially observable: If an agent’s sensors give it access to theFULLY OBSERVABLE PARTIALLY OBSERVABLE complete state of the environment at each point in time, then we say that the task environ- ment is fully observable. A task environment is effectively fully observable if the sensors detect all aspects that are relevant to the choice of action; relevance, in turn, depends on the performance measure. Fully observable environments are convenient because the agent need not maintain any internal state to keep track of the world. An environment might be partially observable because of noisy and inaccurate sensors or because parts of the state are simply missing from the sensor data—for example, a vacuum agent with only a local dirt sensor cannot tell whether there is dirt in other squares, and an automated taxi cannot see what other drivers are thinking. If the agent has no sensors at all then the environment is unobserv- able. One might think that in such cases the agent’s plight is hopeless, but, as we discuss inUNOBSERVABLE Chapter 4, the agent’s goals may still be achievable, sometimes with certainty.

Single agent vs. multiagent: The distinction between single-agent and multiagent environments may seem simple enough. For example, an agent solving a crossword puzzle by itself is clearly in a single-agent environment, whereas an agent playing chess is in a two- agent environment. There are, however, some subtle issues. First, we have described how an entity may be viewed as an agent, but we have not explained which entities must be viewed as agents. Does an agent A (the taxi driver for example) have to treat an object B (another vehicle) as an agent, or can it be treated merely as an object behaving according to the laws of physics, analogous to waves at the beach or leaves blowing in the wind? The key distinction is whether B’s behavior is best described as maximizing a performance measure whose value depends on agent A’s behavior. For example, in chess, the opponent entity B is trying to maximize its performance measure, which, by the rules of chess, minimizes agent A’s per- formance measure. Thus, chess is a competitive multiagent environment. In the taxi-drivingCOMPETITIVE environment, on the other hand, avoiding collisions maximizes the performance measure of all agents, so it is a partially cooperative multiagent environment. It is also partially com-COOPERATIVE petitive because, for example, only one car can occupy a parking space. The agent-design problems in multiagent environments are often quite different from those in single-agent en- vironments; for example, communication often emerges as a rational behavior in multiagent environments; in some competitive environments, randomized behavior is rational because it avoids the pitfalls of predictability.

Deterministic vs. stochastic. If the next state of the environment is completely deter-DETERMINISTIC STOCHASTIC mined by the current state and the action executed by the agent, then we say the environment is deterministic; otherwise, it is stochastic. In principle, an agent need not worry about uncer- tainty in a fully observable, deterministic environment. (In our definition, we ignore uncer- tainty that arises purely from the actions of other agents in a multiagent environment; thus, a game can be deterministic even though each agent may be unable to predict the actions of the others.) If the environment is partially observable, however, then it could appear to be stochastic. Most real situations are so complex that it is impossible to keep track of all the unobserved aspects; for practical purposes, they must be treated as stochastic. Taxi driving is clearly stochastic in this sense, because one can never predict the behavior of traffic exactly; moreover, one’s tires blow out and one’s engine seizes up without warning. The vacuum world as we described it is deterministic, but variations can include stochastic elements such as randomly appearing dirt and an unreliable suction mechanism (Exercise 2.13). We say an environment is uncertain if it is not fully observable or not deterministic. One final note:UNCERTAIN our use of the word “stochastic” generally implies that uncertainty about outcomes is quan- tified in terms of probabilities; a nondeterministic environment is one in which actions areNONDETERMINISTIC characterized by their possible outcomes, but no probabilities are attached to them. Nonde- terministic environment descriptions are usually associated with performance measures that require the agent to succeed for all possible outcomes of its actions.

Episodic vs. sequential: In an episodic task environment, the agent’s experience isEPISODIC SEQUENTIAL divided into atomic episodes. In each episode the agent receives a percept and then performs a single action. Crucially, the next episode does not depend on the actions taken in previous episodes. Many classification tasks are episodic. For example, an agent that has to spot defective parts on an assembly line bases each decision on the current part, regardless of previous decisions; moreover, the current decision doesn’t affect whether the next part is defective. In sequential environments, on the other hand, the current decision could affect all future decisions. 3 Chess and taxi driving are sequential: in both cases, short-term actions can have long-term consequences. Episodic environments are much simpler than sequential environments because the agent does not need to think ahead.

Static vs. dynamic: If the environment can change while an agent is deliberating, thenSTATIC DYNAMIC we say the environment is dynamic for that agent; otherwise, it is static. Static environments are easy to deal with because the agent need not keep looking at the world while it is deciding on an action, nor need it worry about the passage of time. Dynamic environments, on the other hand, are continuously asking the agent what it wants to do; if it hasn’t decided yet, that counts as deciding to do nothing. If the environment itself does not change with the passage of time but the agent’s performance score does, then we say the environment is semidynamic. Taxi driving is clearly dynamic: the other cars and the taxi itself keep movingSEMIDYNAMIC while the driving algorithm dithers about what to do next. Chess, when played with a clock, is semidynamic. Crossword puzzles are static.

Discrete vs. continuous: The discrete/continuous distinction applies to the state of theDISCRETE CONTINUOUS environment, to the way time is handled, and to the percepts and actions of the agent. For example, the chess environment has a finite number of distinct states (excluding the clock). Chess also has a discrete set of percepts and actions. Taxi driving is a continuous-state and continuous-time problem: the speed and location of the taxi and of the other vehicles sweep through a range of continuous values and do so smoothly over time. Taxi-driving actions are also continuous (steering angles, etc.). Input from digital cameras is discrete, strictly speak- ing, but is typically treated as representing continuously varying intensities and locations.

Known vs. unknown: Strictly speaking, this distinction refers not to the environmentKNOWN UNKNOWN itself but to the agent’s (or designer’s) state of knowledge about the “laws of physics” of the environment. In a known environment, the outcomes (or outcome probabilities if the environment is stochastic) for all actions are given. Obviously, if the environment is unknown, the agent will have to learn how it works in order to make good decisions. Note that the distinction between known and unknown environments is not the same as the one between fully and partially observable environments. It is quite possible for a known environment to be partially observable—for example, in solitaire card games, I know the rules but am still unable to see the cards that have not yet been turned over. Conversely, an unknown environment can be fully observable—in a new video game, the screen may show the entire game state but I still don’t know what the buttons do until I try them.

As one might expect, the hardest case is partially observable, multiagent, stochastic, sequential, dynamic, continuous, and unknown. Taxi driving is hard in all these senses, except that for the most part the driver’s environment is known. Driving a rented car in a new country with unfamiliar geography and traffic laws is a lot more exciting.


| Task Environment          | Observable | Agents | Deterministic | Episodic   | Static  | Discrete   |
| ------------------------- | ---------- | ------ | ------------- | ---------- | ------- | ---------- |
| Crossword puzzle          | Fully      | Single | Deterministic | Sequential | Static  | Discrete   |
| Chess with a clock        | Fully      | Multi  | Deterministic | Sequential | Semi    | Discrete   |
| Poker                     | Partially  | Multi  | Stochastic    | Sequential | Static  | Discrete   |
| Backgammon                | Fully      | Multi  | Stochastic    | Sequential | Static  | Discrete   |
| Taxi driving              | Partially  | Multi  | Stochastic    | Sequential | Dynamic | Continuous |
| Medical diagnosis         | Partially  | Single | Stochastic    | Sequential | Dynamic | Continuous |
| Image analysis            | Fully      | Single | Deterministic | Episodic   | Semi    | Continuous |
| Part-picking robot        | Partially  | Single | Stochastic    | Episodic   | Dynamic | Continuous |
| Refinery controller       | Partially  | Single | Stochastic    | Sequential | Dynamic | Continuous |
| Interactive English tutor | Partially  | Multi  | Stochastic    | Sequential | Dynamic | Discrete   |
>Figure 2.6 Examples of task environments and their characteristics.

Figure 2.6 lists the properties of a number of familiar environments. Note that the answers are not always cut and dried. For example, we describe the part-picking robot as episodic, because it normally considers each part in isolation. But if one day there is a large batch of defective parts, the robot should learn from several observations that the distribution of defects has changed, and should modify its behavior for subsequent parts. We have not included a “known/unknown” column because, as explained earlier, this is not strictly a prop- erty of the environment. For some environments, such as chess and poker, it is quite easy to supply the agent with full knowledge of the rules, but it is nonetheless interesting to consider how an agent might learn to play these games without such knowledge.

Several of the answers in the table depend on how the task environment is defined. We have listed the medical-diagnosis task as single-agent because the disease process in a patient is not profitably modeled as an agent; but a medical-diagnosis system might also have to deal with recalcitrant patients and skeptical staff, so the environment could have a multiagent aspect. Furthermore, medical diagnosis is episodic if one conceives of the task as selecting a diagnosis given a list of symptoms; the problem is sequential if the task can include proposing a series of tests, evaluating progress over the course of treatment, and so on. Also, many environments are episodic at higher levels than the agent’s individual actions. For example, a chess tournament consists of a sequence of games; each game is an episode because (by and large) the contribution of the moves in one game to the agent’s overall performance is not affected by the moves in its previous game. On the other hand, decision making within a single game is certainly sequential.

The code repository associated with this book (aima.cs.berkeley.edu) includes imple- mentations of a number of environments, together with a general-purpose environment simu- lator that places one or more agents in a simulated environment, observes their behavior over time, and evaluates them according to a given performance measure. Such experiments are often carried out not for a single environment but for many environments drawn from an en- vironment class. For example, to evaluate a taxi driver in simulated traffic, we would want toENVIRONMENT CLASS run many simulations with different traffic, lighting, and weather conditions. If we designed the agent for a single scenario, we might be able to take advantage of specific properties of the particular case but might not identify a good design for driving in general. For this reason, the code repository also includes an environment generator for each environmentENVIRONMENT GENERATOR class that selects particular environments (with certain likelihoods) in which to run the agent. For example, the vacuum environment generator initializes the dirt pattern and agent location randomly. We are then interested in the agent’s average performance over the environment class. A rational agent for a given environment class maximizes this average performance. Exercises 2.8 to 2.13 take you through the process of developing an environment class and evaluating various agents therein.